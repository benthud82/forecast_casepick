---
output:
  html_document: default
  pdf_document: default
  word_document: default
---


#R Statistics Demo - Model Comparison

Goal of demo is to accurately predict Sparks case pick volume by hour/day.

Dependent variable: boxes received


Independent variables:

* Hour of Day
* Workday of Month - business day of month (weekends are excluded) 
* Week of Month - week number of month 
* Week Day - day number of week (2=mon, 6=fri)
* Day of Month - day of month (weekends are included)
* Month of Year - Month number of year (1-12)
* Year
* Before Vacation - Is this a day before holiday? (1=yes, 0=no)
* After Vacation - Is this a day after a holiday? (1=yes, 0=no)
* Before Chirstmas - Is this the day before Christmas? (1=yes, 0=no) 
* After Christmas - Is this the day after Christmas? (1=yes, 0=no)


The following demo will test 5 predictive models:

1.  Multiple Linear Regression
2.  Decision Tree
3.  Random Forest
4.  Neural Net
5.  Boosted Tree

The measuring statistic will be root mean squared error (RMSE).


## Making Connection to database
```{r echo=TRUE, message=FALSE, warning=FALSE}
knitr::opts_chunk$set(message = FALSE)
packages <- c('useful', 'coefplot', 'xgboost', 'here', 'magrittr', 'dygraphs', 'dplyr', 'RMySQL', 'caret', 'purrr', 'randomForest', 'rpart', 'neuralnet', 'tictoc', 'tinytex', 'DT', 'partykit', 'rpart.plot','rattle')
purrr::walk(packages, library, character.only = TRUE)
#lapply( dbListConnections( dbDriver( drv = "MySQL")), dbDisconnect)
#dev
#mychannel <- dbConnect(MySQL(), user="bentley", pass="dave41", host="127.0.0.1")

#NY Server Prod
mychannel <- dbConnect(MySQL(), user="root", pass="dave41", host="127.0.0.1")

#Google Prod
#mychannel <- dbConnect(MySQL(), user="bentley", pass="dave41", host="104.154.153.225")

query <- function(...) dbGetQuery(mychannel, ...)
  
```

## Workday Table

This table contains all of the dependent variables used to determine boxes shipped.
```{r}
var_whse <- 3
var_build <- 2

sqlquery <- paste("SELECT * FROM printvis.workdayofweek WHERE workday_date >= '2018-06-01'", sep = "")
data_workday <- query(sqlquery)

head(data_workday)
#data_workday


```



##Load historical data

Pull in historical data to determine boxes shipped by hour.  Store in table with all dependent variables.

```{r echo=TRUE, message=FALSE, warning=FALSE}


sqlquery <- paste("SELECT
                  CASE
                  WHEN predicted_availhour < 6 THEN 6
                  WHEN predicted_availhour > 16 THEN 16
                  ELSE predicted_availhour
                  END AS HOUR,
                  workday_workday AS WORKDAY,
                  workday_weekofmon AS MONTHWEEK,
                  workday_weekday AS WEEKDAY,
                  workday_dayofmon AS MONTHDAY,
                  workday_month AS MONTH,
                  YEAR(predicted_availdate) as YEAR,
                  workday_befvac AS BEFVAC,
                  workday_aftvac AS AFTVAC,
                  workday_befchrist AS BEFCHR,
                  workday_aftchrist AS AFTCHR,
                  SUM(CASE
                  WHEN
                  hist_twoday = 1
                  AND predicted_availhour < 6
                  THEN
                  1
                  ELSE 1
                  END) AS BOXES
                  FROM
                  printvis.hist_casevol
                  JOIN
                  printvis.workdayofweek ON predicted_availdate = workday_date
                  WHERE
                  hist_whse = ",var_whse," and hist_build = ",var_build,"
                  AND hist_equip = 'PALLETJACK'
                  AND cutoff_group NOT IN ('TRUCK' , 'COLGATE')
                  AND predicted_availhour BETWEEN 7 AND 16
                  GROUP BY predicted_availdate , hist_equip,
                  CASE
                  WHEN predicted_availhour < 6 THEN 6
                  WHEN predicted_availhour > 16 THEN 16
                  ELSE predicted_availhour
                  END
                  ORDER BY predicted_availdate , CASE
                  WHEN predicted_availhour < 6 THEN 6
                  WHEN predicted_availhour > 16 THEN 16
                  ELSE predicted_availhour
                  END", sep = "")
data <- query(sqlquery)

#DT::datatable(head(data))
head(data)

```


##Set seed and determine training and test data

```{r echo=TRUE}

set.seed(222)
trainIndex <- createDataPartition(data$BOXES,
                                  p = .75,
                                  list = FALSE,
                                  times = 1)

dataTrain <- data[ trainIndex,]
dataTest  <- data[-trainIndex,]

#DT::datatable(head(dataTrain))
#DT::datatable(head(dataTest))
head(dataTrain)
head(dataTest)

data_formula_boxes <- BOXES ~ HOUR + WORKDAY + MONTHWEEK + WEEKDAY + MONTHDAY + MONTH + YEAR + BEFVAC + AFTVAC + BEFCHR + AFTCHR

#boxes data training
dataX_Train_box <- build.x(data_formula_boxes, data=dataTrain,
                       contrasts=FALSE,
                       sparse=TRUE)
dataY_Train_box <- build.y(data_formula_boxes,data=dataTrain)

dataX_Test_box <- build.x(data_formula_boxes, data=dataTest,
                      contrasts=FALSE,
                      sparse=TRUE)
dataY_Test_box <- build.y(data_formula_boxes,data=dataTest)

xgTrain_box <- xgb.DMatrix(data=dataX_Train_box,
                       label=dataY_Train_box)

xgVal_box <- xgb.DMatrix(data=dataX_Test_box,
                     label=dataY_Test_box)
```

#Create the Multiple Linear regression model
![Linear Regression - Least Squares]

Multiple linear regression attempts to model the relationship between two or more explanatory variables and a response variable by fitting a linear equation to observed data

###Advantages
* Easy to use and understand
* Works with most business cases

###Disadvantages
* Relationship between variables must be linear
* Very sensitive to outliers
* Problem of over-fitting to sample data
* Every attribute receives a weighting


```{r echo=FALSE}
model.lm <- lm(data_formula_boxes, data=dataTrain)
prediction.lm <- predict(model.lm, newdata=dataTest)
rmse.lm <- sqrt(mean((dataTest$BOXES-prediction.lm)^2))
rmse.lm
plot(dataTest$BOXES,prediction.lm,col='blue',main='Real vs predicted Linear Regression',pch=18, cex=0.7)
abline(0,1,lwd=2)

```


#Create the decision tree model



###Advantages
* Can understand what variables impact dependent variable
* Resistant to irrelevant attributes
* Resistant to outliers

###Disadvantages
* High tendency to over-fit to sample data
* Can be complex


```{r echo=TRUE}

model.dt <- rpart(data_formula_boxes, method = "anova", data=dataTrain, minsplit = 100)
prediction.dt <- predict(model.dt, newdata = dataTest)
rmse.dt <- sqrt(mean((dataTest$BOXES-prediction.dt)^2))
rmse.dt
plot(dataTest$BOXES,prediction.dt,col='blue',main='Real vs predicted Decision Tree',pch=18, cex=0.7)
abline(0,1,lwd=2)

fancyRpartPlot((model.dt))

```


#Create the random forest model

A random forest is simply a collection of decision trees whose results are aggregated into one final result.

Has many of the the same advantages and disadvantages.  

However, random forest addresses the overfitting through a process called "bagging".  The basic idea is to resample the data over and over and for each sample train a new classifier. Different classifiers overfit the data in a different way, and through voting those differences are averaged out.

```{r}

dataTrain_fac <- dataTrain %>% mutate_if(is.character, as.factor)
dataTest_fac <- dataTest %>% mutate_if(is.character, as.factor)
model.rf <- randomForest(data_formula_boxes, data = dataTrain_fac)
prediction.rf <- predict(model.rf, newdata = dataTest_fac)
rmse.rf <- sqrt(mean((dataTest_fac$BOXES-prediction.rf)^2))
rmse.rf
plot(dataTest$BOXES,prediction.rf,col='blue',main='Real vs predicted Random Forest',pch=18, cex=0.7)
abline(0,1,lwd=2)

```


#Create the neural net model
```{r}
# library('neuralnet')
# 
# maxs <- apply(dataTrain, 2, max)
# mins <- apply(dataTrain, 2, min)
# scaled_train <- as.data.frame(scale(dataTrain, center = mins, scale = maxs - mins))
# scaled_test <- as.data.frame(scale(dataTest, center = mins, scale = maxs - mins))
# n <- names(scaled_train)
# f <- as.formula(paste("BOXES ~", paste(n[!n %in% "BOXES"], collapse = " + ")))
# 
# nn <- neuralnet(f,data=scaled_train,hidden=c(8,4),linear.output=F)
# pr.nn <- compute(nn,scaled_test[,0:11])
# pr.nn_ <- pr.nn$net.result*(max(data$BOXES)-min(data$BOXES))+min(data$BOXES)
# test.r <- (scaled_test$BOXES)*(max(data$BOXES)-min(data$BOXES))+min(data$BOXES)
# rmse.nn <- sqrt(mean((dataTest$BOXES-pr.nn_)^2))
# rmse.nn
# 
#  plot(dataTest$BOXES,pr.nn_,col='blue',main='Real vs predicted Neural Net',pch=18, cex=0.7)
#  abline(0,1,lwd=2)

```



#Create the Gradient Boosted Tree model (GBM)

GBM is a boosting method, which builds on weak classifiers. The idea is to add a classifier at a time, so that the next classifier is trained to improve the already trained ensemble. Notice that for RF each iteration the classifier is trained independently from the rest.  For GMB, the classifiers are trained in sequence to "boost" the accuracy of the model.

Two outputs will be evaluated.  

* The RMSE on the training data (which will consistantly decrease over sequences).  
* The RMSE on the test data.  
  + The RMSE will decrease substantially on the initial sequences
  + As more sequences are run, the RMSE will decrease slower
  + At some point, the RMSE will start to increase (although the RMSE on the train data will continue to decrease).
  + This indicates that over-fitting of the train data is starting to occur and the sequences should be stopped.


###GBM - Example 1: 1 round, 1 parrallel tree, 1 cpu thread
```{r}

model.xgb <- xgb.train(data=xgTrain_box, objective='reg:linear', eval_metric='rmse', booster='gbtree', watchlist = list(train=xgTrain_box, validate=xgVal_box), nrounds = 1, num_parallel_tree=1, print_every_n = 1, nthread=1,eta = .1, max_depth = 7)

dataTest.xgb <- build.x(data_formula_boxes, data=dataTest, contrasts = FALSE, sparse = TRUE)

prediction.xgb <- predict(model.xgb, newdata = dataTest.xgb)
plot(dataTest$BOXES,prediction.xgb,col='blue',main='Real vs predicted Boosted Forest - Example 1',pch=18, cex=0.7)
 abline(0,1,lwd=2)

```


###GBM - Example 2: 2 rounds, 1 parrallel tree, 1 cpu thread
```{r}

model.xgb <- xgb.train(data=xgTrain_box, objective='reg:linear', eval_metric='rmse', booster='gbtree', watchlist = list(train=xgTrain_box, validate=xgVal_box), nrounds = 2, num_parallel_tree=1, print_every_n = 1, nthread=1,eta = .1, max_depth = 7)

dataTest.xgb <- build.x(data_formula_boxes, data=dataTest, contrasts = FALSE, sparse = TRUE)

prediction.xgb <- predict(model.xgb, newdata = dataTest.xgb)
plot(dataTest$BOXES,prediction.xgb,col='blue',main='Real vs predicted Boosted Forest - Example 2',pch=18, cex=0.7)
 abline(0,1,lwd=2)

```

###GBM - Example 3: 20 rounds, 20 parrallel tree, 1 cpu thread
```{r}

model.xgb <- xgb.train(data=xgTrain_box, objective='reg:linear', eval_metric='rmse', booster='gbtree', watchlist = list(train=xgTrain_box, validate=xgVal_box), nrounds = 20, num_parallel_tree=20, print_every_n = 1, nthread=1,eta = .1, max_depth = 7)

dataTest.xgb <- build.x(data_formula_boxes, data=dataTest, contrasts = FALSE, sparse = TRUE)

prediction.xgb <- predict(model.xgb, newdata = dataTest.xgb)
plot(dataTest$BOXES,prediction.xgb,col='blue',main='Real vs predicted Boosted Forest - Example 3',pch=18, cex=0.7)
 abline(0,1,lwd=2)

```

###GBM - Example 4: 250 rounds, 20 parrallel tree, 4 cpu thread
```{r}

model.xgb <- xgb.train(data=xgTrain_box, objective='reg:linear', eval_metric='rmse', booster='gbtree', watchlist = list(train=xgTrain_box, validate=xgVal_box), nrounds = 250, num_parallel_tree=20, print_every_n = 1, nthread=4,eta = .1, max_depth = 7)

dataTest.xgb <- build.x(data_formula_boxes, data=dataTest, contrasts = FALSE, sparse = TRUE)

prediction.xgb <- predict(model.xgb, newdata = dataTest.xgb)
plot(dataTest$BOXES,prediction.xgb,col='blue',main='Real vs predicted Boosted Forest - Example 4',pch=18, cex=0.7)
 abline(0,1,lwd=2)
dygraph(model.xgb$evaluation_log)
xgb.plot.importance(xgb.importance(model.xgb, feature_names=colnames(dataTrain)))

```




###Final model - uses early stopping rounds
```{r}

model.xgb <- xgb.train(data=xgTrain_box, objective='reg:linear', eval_metric='rmse', booster='gbtree', watchlist = list(train=xgTrain_box, validate=xgVal_box),
                  early_stopping_rounds=100, nrounds = 100000, num_parallel_tree=20, print_every_n = 20, nthread=8,eta = .1, max_depth = 7)

dataTest.xgb <- build.x(data_formula_boxes, data=dataTest, contrasts = FALSE, sparse = TRUE)

prediction.xgb <- predict(model.xgb, newdata = dataTest.xgb)
plot(dataTest$BOXES,prediction.xgb,col='blue',main='Real vs predicted Boosted Forest',pch=18, cex=0.7)
 abline(0,1,lwd=2)

```


#Use model to predict volume next day

##Pull in the dependent variables for the next day

```{r}



```




